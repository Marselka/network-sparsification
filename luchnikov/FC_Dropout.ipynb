{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST('./data', train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST('./data', train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 28*28\n",
    "n2 = 100\n",
    "n3 = 40\n",
    "n4 = 10\n",
    "\n",
    "k1 = torch.FloatTensor([0.63576])\n",
    "k2 = torch.FloatTensor([1.87320])\n",
    "k3 = torch.FloatTensor([1.48695])\n",
    "\n",
    "def kl(log_sigma_2, theta):\n",
    "    log_alpha = log_sigma_2 - torch.log(theta.pow(2))\n",
    "    log_alpha = log_alpha.clamp(-8,8)\n",
    "    return (k1*torch.sigmoid(k2 + k3*log_alpha) - 0.5*torch.log1p(log_alpha.neg().exp()) - k1).sum()\n",
    "\n",
    "def rand_W(n_in, n_out):\n",
    "    return rnd.randn(n_in, n_out)\n",
    "\n",
    "def rand_b(n):\n",
    "    return rnd.randn(n)    \n",
    "    \n",
    "class dropout_fc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dropout_fc, self).__init__()\n",
    "        init_W = lambda a,b: nn.Parameter(torch.FloatTensor(a, b).normal_(mean=0, std=np.sqrt(1/b)))\n",
    "        init_b = lambda b: nn.Parameter(torch.FloatTensor(b).normal_(mean=0, std=np.sqrt(1/b)))\n",
    "        self.W1, self.W2, self.W3 = init_W(n1, n2), init_W(n2, n3), init_W(n3, n4)\n",
    "        self.b1, self.b2, self.b3 = init_b(n2), init_b(n3), init_b(n4)\n",
    "#         self.W1 = nn.Parameter(torch.FloatTensor(n1, n2).normal_(mean=0, std=np.sqrt(1/n2)))\n",
    "#         self.b1 = nn.Parameter(torch.FloatTensor(n2).normal_(mean=0, std=np.sqrt(1/n2)))\n",
    "#         self.W2 = nn.Parameter(torch.FloatTensor(n2, n3).normal_(mean=0, std=np.sqrt(1/n3)))\n",
    "#         self.b2 = nn.Parameter(torch.FloatTensor(n3).normal_(mean=0, std=np.sqrt(1/n3)))\n",
    "#         self.W3 = nn.Parameter(torch.FloatTensor(n3, n4).normal_(mean=0, std=np.sqrt(1/n4)))\n",
    "#         self.b3 = nn.Parameter(torch.FloatTensor(n4).normal_(mean=0, std=np.sqrt(1/n4)))\n",
    "        \n",
    "        init_W_drop = lambda a,b: nn.Parameter(torch.FloatTensor(a, b).fill_(-11.5))\n",
    "        init_b_drop = lambda a: nn.Parameter(torch.FloatTensor(a).fill_(-11.5))\n",
    "        self.W1_drop, self.W2_drop, self.W3_drop = init_W_drop(n1, n2), init_W_drop(n2, n3), init_W_drop(n3, n4)\n",
    "        self.b1_drop, self.b2_drop, self.b3_drop = init_b_drop(n2), init_b_drop(n3), init_b_drop(n4)\n",
    "        #self.W1_drop = nn.Parameter(torch.FloatTensor(n1, n2).fill_(-5))\n",
    "        #self.b1_drop = nn.Parameter(torch.FloatTensor(n2).fill_(-5))\n",
    "        #self.W2_drop = nn.Parameter(torch.FloatTensor(n2, n3).fill_(-5))\n",
    "        #self.b2_drop = nn.Parameter(torch.FloatTensor(n3).fill_(-5))\n",
    "        #self.W3_drop = nn.Parameter(torch.FloatTensor(n3, n4).fill_(-5))\n",
    "        #self.b3_drop = nn.Parameter(torch.FloatTensor(n4).fill_(-5))\n",
    "        \n",
    "    def forward(self, x, lbls, eW1, eW2, eW3, eb1, eb2, eb3, reg):\n",
    "        x = x.view(-1, 28*28)\n",
    "        W1_resulting = self.W1.add(1e-8) + torch.exp(self.W1_drop.clamp(-15,15).div(2)).mul(torch.FloatTensor(eW1))\n",
    "        W2_resulting = self.W2.add(1e-8) + torch.exp(self.W2_drop.clamp(-15,15).div(2)).mul(torch.FloatTensor(eW2))\n",
    "        W3_resulting = self.W3.add(1e-8) + torch.exp(self.W3_drop.clamp(-15,15).div(2)).mul(torch.FloatTensor(eW3))\n",
    "        b1_resulting = self.b1.add(1e-8) + torch.exp(self.b1_drop.clamp(-15,15).div(2)).mul(torch.FloatTensor(eb1))\n",
    "        b2_resulting = self.b2.add(1e-8) + torch.exp(self.b2_drop.clamp(-15,15).div(2)).mul(torch.FloatTensor(eb2))\n",
    "        b3_resulting = self.b3.add(1e-8) + torch.exp(self.b3_drop.clamp(-15,15).div(2)).mul(torch.FloatTensor(eb3))\n",
    "        \n",
    "        x = F.relu(x.mm(W1_resulting) + b1_resulting)\n",
    "        x = F.relu(x.mm(W2_resulting) + b2_resulting)\n",
    "        x = F.log_softmax(x.mm(W3_resulting) + b3_resulting)\n",
    "        \n",
    "        kl_W1 = kl(self.W1_drop, self.W1)\n",
    "        kl_W2 = kl(self.W2_drop, self.W2)\n",
    "        kl_W3 = kl(self.W3_drop, self.W3)\n",
    "        kl_b1 = kl(self.b1_drop, self.b1)\n",
    "        kl_b2 = kl(self.b2_drop, self.b2)\n",
    "        kl_b3 = kl(self.b3_drop, self.b3)\n",
    "        \n",
    "        loss = len(train_loader)*F.nll_loss(x, lbls) - reg*(kl_W1 + kl_W2 + kl_W3 + kl_b1 + kl_b2 + kl_b3)\n",
    "        return loss\n",
    "    \n",
    "    def pred(self, x):\n",
    "        #sol = torch.FloatTensor([0.])\n",
    "        x = x.view(-1, 28*28)\n",
    "        #for r in range(N):\n",
    "        #    eW1 = rand_W(n1, n2)\n",
    "        #    eW2 = rand_W(n2, n3)\n",
    "        #    eW3 = rand_W(n3, n4)\n",
    "        #    eb1 = rand_b(n2)\n",
    "        #    eb2 = rand_b(n3)\n",
    "        #    eb3 = rand_b(n4)\n",
    "        #    W1_resulting = self.W1 + torch.exp(self.W1_drop.div(2)).mul(torch.FloatTensor(eW1))\n",
    "        #    W2_resulting = self.W2 + torch.exp(self.W2_drop.div(2)).mul(torch.FloatTensor(eW2))\n",
    "        #    W3_resulting = self.W3 + torch.exp(self.W3_drop.div(2)).mul(torch.FloatTensor(eW3))\n",
    "        #    b1_resulting = self.b1 + torch.exp(self.b1_drop.div(2)).mul(torch.FloatTensor(eb1))\n",
    "        #    b2_resulting = self.b2 + torch.exp(self.b2_drop.div(2)).mul(torch.FloatTensor(eb2))\n",
    "        #    b3_resulting = self.b3 + torch.exp(self.b3_drop.div(2)).mul(torch.FloatTensor(eb3))\n",
    "            \n",
    "        y = F.relu(x.mm(self.W1) + self.b1)\n",
    "        y = F.relu(y.mm(self.W2) + self.b2)\n",
    "        #y_max = torch.max(y)\n",
    "        #y = y - y_max\n",
    "        #y = y.clamp(-10,0.1)\n",
    "        y = F.softmax(y.mm(self.W3) + self.b3)\n",
    "        return torch.argmax(y, dim = 1)\n",
    "    \n",
    "def epoch(model, opt, reg, ep_number, test):\n",
    "    loss_history = np.array([])\n",
    "    for batch_id, (data, target) in enumerate(train_loader):\n",
    "        loss = model.forward(data, target, rand_W(n1, n2), rand_W(n2, n3), rand_W(n3, n4), rand_b(n2), rand_b(n3), rand_b(n4), reg)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_history = np.append(loss_history, loss.data.numpy())\n",
    "        spars = round((model.W1.data.numel() + model.W2.data.numel() + model.W3.data.numel())/((model.W1.data>0.001).sum() + (model.W2.data>0.001).sum() + (model.W3.data>0.001).sum()).item(),1)\n",
    "        if batch_id%60 == 0:\n",
    "            #fig = plt.figure()\n",
    "            #fig.set_size_inches(15., 6.)\n",
    "            #fig.subtitle('efwdf')\n",
    "            #plt.subplot(1, 3, 1)\n",
    "            #plt.title('$W1$')\n",
    "            #plt.imshow(clf.W1.data.numpy(), cmap=cm.seismic)\n",
    "            #plt.colorbar(ticks=[-1, 0, 1])\n",
    "            #plt.clim(-1, 1)\n",
    "            #plt.subplot(1, 3, 2)\n",
    "            #plt.title('$W2$')\n",
    "            #plt.imshow(clf.W2.data.numpy(), cmap=cm.seismic)\n",
    "            #plt.colorbar(ticks=[-1, 0, 1])\n",
    "            #plt.clim(-1, 1)\n",
    "            #plt.subplot(1, 3, 3)\n",
    "            #plt.title('$W3$')\n",
    "            #plt.imshow(clf.W3.data.numpy(), cmap=cm.seismic)\n",
    "            #plt.colorbar(ticks=[-1, 0, 1])\n",
    "            #plt.clim(-1, 1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            fig, axarr = plt.subplots(1, 3)\n",
    "            \n",
    "            fig.set_size_inches(15., 6.)\n",
    "            fig.suptitle(f\"Accuracy$=${test}, Compression$=${spars}\", fontsize=16)\n",
    "\n",
    "            fig1 = axarr[0].imshow(model.W1.data.numpy(), cmap=cm.seismic)\n",
    "            fig1.set_clim(-1,1)\n",
    "            axarr[0].set_title('$W_1$')\n",
    "            cb1 = fig.colorbar(fig1, ax=axarr[0], ticks=[-1, 0, 1])\n",
    "            fig2 = axarr[1].imshow(model.W2.data.numpy(), cmap=cm.seismic)\n",
    "            fig2.set_clim(-1,1)\n",
    "            axarr[1].set_title('$W_2$')\n",
    "            cb2 = fig.colorbar(fig2, ax=axarr[1])\n",
    "            fig3 = axarr[2].imshow(model.W3.data.numpy(), cmap=cm.seismic)\n",
    "            fig3.set_clim(-1,1)\n",
    "            axarr[2].set_title('$W_3$')\n",
    "            cb3 = fig.colorbar(fig3, ax=axarr[2])\n",
    "            \n",
    "            \n",
    "            plt.savefig(f'fc_drop/foo{batch_id/60+ep_number*10}.png')\n",
    "    return loss_history.sum()/(batch_size*len(train_loader))\n",
    "\n",
    "def test(model):\n",
    "    correct = 0\n",
    "    for batch_id, (data, target) in enumerate(test_loader):\n",
    "        predicted = model.pred(data)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    return 100*correct/(len(test_loader)*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilalucnikov/miniconda3/lib/python3.7/site-packages/ipykernel/__main__.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/ilalucnikov/miniconda3/lib/python3.7/site-packages/ipykernel/__main__.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "clf = dropout_fc()\n",
    "opt = optim.Adam(clf.parameters(), lr=0.0003)\n",
    "loss_train = np.array([])\n",
    "loss_test = np.array([])\n",
    "reg = 1\n",
    "for k in range(100):\n",
    "    #if k>5:\n",
    "    #   reg = 1\n",
    "    #loss_train = np.append(loss_train, epoch(clf, opt))\n",
    "    #loss_train = np.append(loss_test, test(clf))\n",
    "    tst = test(clf)\n",
    "    epoch(clf, opt, reg, k, tst)\n",
    "    print(tst)\n",
    "    #print(test(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "filenames = [f'fc_drop/foo{i}.0.png' for i in range(82)]\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('fc_drop/movie.gif', images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
